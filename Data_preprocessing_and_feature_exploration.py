# -*- coding: utf-8 -*-
"""Data_preprocessing_and_feature_exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wx3b4q9KTDo1gvpTnb2OUKbE8WXXOAhH

### Importing required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries required

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt #for plotting of graphs
import seaborn as sns# for plotting of graphs
# %matplotlib inline


np.random.seed(2)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop, Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau


sns.set(style='white', context='notebook', palette='deep')

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle competitions list

!kaggle competitions download -c digit-recognizer # downloading kaggle digit recognizer

# unzip the dataset
!unzip -q "../content/train.csv.zip"
!unzip -q "../content/test.csv.zip"

#deleting the zip files
!rm -rf test.csv.zip
!rm -rf train.csv.zip

# dataset read
train = pd.read_csv("../content/train.csv")
test = pd.read_csv("../content/test.csv")
train.head() #printing first five rows

train.shape
y_train = train["label"] #copy the label column of train dataset to y_train dataset
#drop label column
x_train = train.drop(labels=["label"],axis=1)
#plotting count plot for the digits labelled 
g =sns.countplot(y_train)
y_train.value_counts()

train.shape
# shape of the the dataset

y_train[0]
# number present at the first label

print(x_train.shape)
print(y_train.shape)
print(test.shape)
#print shape of all datasets

"""### Checking for Missing Values

"""

# Check the data
x_train.isnull().any().describe()

test.isnull().any().describe()

"""I check for corrupted images (missing values inside).

There is no missing values in the train and test dataset.

### Reshape
"""

# For each pixel you would need 3 scalars (each for one channel), so it would be 60000x28x28x3.
# And how many channels you need when the image is in greyscale? Just one, so it would be 60000x28x28x1
# x_train = x_train.values.reshape(42000,28,28,1)
import numpy as np
x_train = x_train.values.reshape(-1,28,28,1)
print(x_train.shape)
test = test.values.reshape(-1,28,28,1)
print(test.shape)

"""### Normalisation
Normalising data by dividing it by 255 should improve activation functions performance - sigmoid function works more efficiently with data range 0.0-1.0
"""

x_train = x_train/255.0
test = test/255.0

"""### Label encoding"""

#one-hot encoding
# encode the data to convert labels into 10 numbers for input to neural networks
y_train = to_categorical(y_train,num_classes =10)
y_train.shape

print(train['label'].head())
print(y_train[0:5,:])

"""### Split training and valdiation set """

# Split the train and the validation set for the fitting
X_train,X_val,Y_train,Y_val = train_test_split(x_train,y_train,test_size = 0.1 , random_state = 2)

print("X Train shape",X_train.shape)
print("Y Train shape",Y_train.shape)
print("X Test shape ",X_val.shape)
print("Y test shape ",Y_val.shape)

g = plt.imshow(X_train[0][:,:,0])

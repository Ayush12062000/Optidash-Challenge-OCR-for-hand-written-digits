# -*- coding: utf-8 -*-
"""train_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jro7N9Bdy_Xgj9Wd9mHMl1ucLwi6JJU4

# Fetching dataset
"""

import keras
import matplotlib.pyplot as plt
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

y_train_catagorical = keras.utils.to_categorical(y_train, 10)
y_test_catagorical = keras.utils.to_categorical(y_test, 10)

X_train_expanded =  np.expand_dims(X_train, axis=-1)
X_test_expanded =  np.expand_dims(X_test, axis=-1)

X_train_classifier, X_val_classifier, y_train_classifier, y_val_classifier = train_test_split(X_train_expanded, y_train_catagorical, test_size = 0.1, random_state = 1)

X_test_classifier = X_test_expanded
y_test_classifier = y_test_catagorical

"""## Vizualizing the Dataset"""

plt.imshow(X_train[101])

y_train[101]

"""# Classifier"""

from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Dropout

classifier = Sequential()

classifier.add(Conv2D(32, (3, 3), input_shape = (28, 28, 1), activation = 'relu', kernel_initializer='random_normal'))
classifier.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer = 'random_normal'))
classifier.add(MaxPooling2D(pool_size = (2,2)))
classifier.add(Dropout(0.6))

classifier.add(Conv2D(64, (3,3), activation = 'relu', kernel_initializer = 'random_normal'))
classifier.add(Conv2D(64, (3,3), activation = 'relu', kernel_initializer = 'random_normal'))
classifier.add(MaxPooling2D(pool_size = (2,2)))
classifier.add(Dropout(0.6))

classifier.add(Flatten())

classifier.add(Dense(units = 128, activation = 'relu', kernel_initializer='random_normal'))
classifier.add(Dropout(0.6))

classifier.add(Dense(units = 10, activation = 'softmax', kernel_initializer='random_normal'))

opt = keras.optimizers.Adam(0.001)

classifier.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])

"""Model"""

classifier.summary()

"""Training Model"""

callbacks = [
    keras.callbacks.ModelCheckpoint('classifier.h5', save_best_only=True),
    keras.callbacks.ReduceLROnPlateau()]

history = classifier.fit(X_train_classifier, y_train_classifier, batch_size=32, epochs=100, verbose=1, validation_data = (X_val_classifier, y_val_classifier), callbacks = callbacks)

"""Testing the Dataset"""

model = keras.models.load_model("classifier.h5")

y_true = np.argmax(y_test_classifier,axis=1)
y_p = model.predict(X_test_classifier)
y_predicted = np.argmax(y_p,axis=1)
print('confusion matrix')
print(confusion_matrix(y_true,y_predicted))

print('Classification report')
print(classification_report(y_true,y_predicted))

"""Training Evaluation"""

# summarize history for accuracy
plt.plot(history.history['accuracy'][0:100])
plt.plot(history.history['val_accuracy'][0:100])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training_accuracy', 'validation_accuracy'], loc='lower right')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'][0:100])
plt.plot(history.history['val_loss'][0:100])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['training_accuracy', 'validation_accuracy'], loc='upper right')
plt.show()
